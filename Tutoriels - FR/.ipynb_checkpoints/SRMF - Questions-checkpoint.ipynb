{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÉCOLE IVADO - SYSTÈMES DE RECOMMANDATION\n",
    "# ÉTÉ 2019 \n",
    "\n",
    "# Factorisation matricielle\n",
    "\n",
    "## Auteurs: \n",
    "\n",
    "David Berger (davidberger2785 [at] gmail [dot] com)\n",
    "\n",
    "Laurent Charlin (lcharlin [at] gmail [dot] com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Cet atelier se veut une introduction aux systèmes de recommandation. Plus spécifiquement, nous proposons d'implémenter dans un premier temps un système de recommandation basé sur la factorisation matricielle. Ce type d'architecture étant fortement associé à la <a href=\"https://en.wikipedia.org/wiki/Netflix_Prize\"> compétition Netflix</a>, nous utiliserons la base de données <a href=\"https://grouplens.org/datasets/movielens/\"> MovieLens</a> afin d'entraîner nos modèles et de mener certaines expériences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Installation des librairies\n",
    "\n",
    "Avant de commencer, nous devons nous assurer d'installer les librairies nécessaires pour le tutoriel à l'aide de `pip`.  Pour ce faire, exécutez la cellule suivante en la sélectionnant et en cliquant `shift`+`Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de vous assurer que l'installation ait eu lieu, importez toutes les libraries et modules dont nous nous servirons pour cet atelier en exécutant la prochaine cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Écriture fichier\n",
    "import pickle\n",
    "\n",
    "# Regroupement des fonctions maisons\n",
    "import utilities as utl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons également écrit quelques fonctions passe-partout que nous avons regroupées dans la librairie `utilities`. En fait, ces différentes fonctions existent fort probablement déjà en python, mais nous en ignorons simplement l'existence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Objectif\n",
    "\n",
    "De façon générale, l'objectif d'un système de recommandation est, comme son nom l'indique, d'effectuer des recommandations personnalisées à chacun des utilisateurs. Idéalement, ces recommandations devront être bonnes, bien que ce concept puisse rapidement devenir flou. Contrairement à d'autres tâches en apprentissage automatique, telle la reconnaissance d'images de chats ou la prédiction du cours d'une action en bourse, effectuer des recommandations de manière à aider un utilisateur est d'autant plus complexe que ce problème est plus ou moins bien défini. Cherchons-nous à présenter à un utilisateur précis des suggestions le confortant dans ses choix antérieurs? Ou enconre, voulons-nous lui présenter des suggestions complémentaires ou totalement indépendantes des items précédemment considérés? Enfin, tenterons-nous plutôt de lui présenter des items auxquels il n'a pas encore été exposé? Chacune des ces options sont légitimes et pourront être modélisées. Sans perte de généralités, le schéma ci-dessous modélise simplement la problématique des systèmes de recommandation sous l'angle de l'apprentissage automatique.\n",
    "\n",
    "<img src=\"../PIC/High_level_1.png\" width=\"500\">\n",
    "\n",
    "N'empêche, dans le cadre de cet atelier et en considérant le contexte dans lequel nous sommes plongés, soit la suggestion de films comme le font Netflix ou Amazon Prime, nous pouvons réduire le problème à une tâche relativement simple: recommander des films que l'utilisateur va aimer en fonction de ses intérêts passés. Afin de mener à bien cette tâche, nous utiliserons l'ensemble des préférences des usagers, certaines variables sociodémographiques associées de même que certaines caractéristiques des films. Enfin, nous pouvons rafiner le schéma ainsi: \n",
    "\n",
    "<img src=\"../PIC/High_level_2.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Jeu de données  - MoviesLens 100k\n",
    "\n",
    "Les données que nous allons manipuler afin d'explorer différents types de systèmes de recommandation sont celles associées au projet <a href=\"https://grouplens.org/datasets/movielens/\"> MovieLens</a>. Brièvement, les données utilisées consistent ici en plus ou moins 100 000 évaluations de films effectuées par 943 utilisateurs et où un ensemble de 1 682 films étaient disponibles en visionnement. En plus des 100 000 évaluations à notre disposition, nous avons des informations complémentaires liées à chacun des usagers de même qu'à chacun des films.\n",
    "\n",
    "En somme, nous allons utiliser un total de trois jeux de données afin de mener à bien nos analyses soit: \n",
    "\n",
    "<ul>\n",
    "<li> Users : contenant des informations associées aux caractéristiques des utilisateurs,\n",
    "<li> Movies : contenant des informations associées aux caractéristiques des films en visionnement,\n",
    "<li> Ratings : contenant l'ensemble des 100 000 évaluations effectuées par les utilisateurs.\n",
    "\n",
    "Nous utiliserons la librarire <a href=\"https://pandas.pydata.org/\">Pandas</a> pour télécharger la base de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Users: Importation et prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement des données\n",
    "users = pd.read_csv('../data/ml-100k/u.user', sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "# Nous définissons les différentes variables en fonction de l'information fournie dans le fichier 'readme'\n",
    "users.columns = ['Index', 'Age', 'Gender', 'Occupation', 'Zip code']\n",
    "\n",
    "# Sauvegarde des données traitées\n",
    "pickle.dump(users, open('../data/saves/users', 'wb'))\n",
    "\n",
    "# Bref aperçu\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de présenter les statistiques descriptives liées à la population étudiée, nous allons dans un premier temps traiter les données associées aux usagers sous la forme d'une <a href=\"https://fr.wikipedia.org/wiki/Liste_(informatique)\"> liste</a> afin de pouvoir plus aisément les manipuler. Notons que l'occupation de chaque individu étant une chaîne de caractères, nous avons recodé les 21 occupations possibles en booléen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'utilisateurs et d'utilisatrices\n",
    "nb_users = len(users)\n",
    "\n",
    "# Sexe\n",
    "gender = np.where(np.matrix(users['Gender']) == 'M', 0, 1)[0]\n",
    "\n",
    "# Occupation\n",
    "occupation_name = np.array(pd.read_csv('../data/ml-100k/u.occupation', \n",
    "                                            sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Recodage en booléen de la variable occupation\n",
    "occupation_matrix = np.zeros((nb_users, len(occupation_name)))\n",
    "\n",
    "for k in np.arange(nb_users):\n",
    "    occupation_matrix[k, occupation_name.tolist().index(users['Occupation'][k])] = 1\n",
    "\n",
    "# Concatenation des différentes données sociodémographiques sous forme de liste\n",
    "user_attributes = np.concatenate((np.matrix(users['Age']), np.matrix(gender), occupation_matrix.T)).T.tolist()\n",
    "pickle.dump(user_attributes, open('../data/saves/user_attributes', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous explorons par la suite les différentes statistiques descriptives associées aux usagers. Celles-ci comportent des informations en lien avec l'âge (variable continue), le sexe (variable binaire) et l'occupation de chacun des usagers (au nombre de 21, toutes binaires). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistiques descriptives associées à l'âge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(users['Age'].describe()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramme à bandes pour la stastistique associée au sexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utl.barplot(['Women', 'Men'], np.array([np.mean(gender) , 1 - np.mean(gender)]) * 100, \n",
    "            'Sex', 'Percentage (%)', \"User's gender\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramme à bandes pour la stastistique associée à l'occupation des individus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(occupation_name, np.mean(occupation_matrix, axis=0) * 100)\n",
    "utl.barplot(attributes, scores, 'Occupation', 'Percentage (%)', \"User's occupation\", 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Movies: Importation et reformatage des données\n",
    "\n",
    "De la même façon, nous allons traiter et explorer les données associées aux films. Pour chacun d'eux, nous disposons du titre, de la date de sortie en Amérique du Nord, de même que les genres auxquels il est associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../data/ml-100k/u.item', sep='|', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "nb_movies = len(movies)\n",
    "\n",
    "movies_genre = np.matrix(movies.loc[:, 5:])\n",
    "movies_genre_name = np.array(pd.read_csv('../data/ml-100k/u.genre', sep='|', header=None, engine='python', encoding='latin-1').loc[:, 0])\n",
    "\n",
    "# Nous définissons les différentes variables en fonction de l'information fournie dans le fichier 'readme'\n",
    "movies.columns = ['Index', 'Title', 'Release', 'The Not a Number column', 'Imdb'] + movies_genre_name.tolist()\n",
    "pickle.dump(movies, open('../data/saves/movies', 'wb'))\n",
    "\n",
    "# Bref aperçu\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous présentons les proportions de films en fonction du genre comme statistique descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, scores = utl.rearrange(movies_genre_name, \n",
    "                                   np.array(np.round(np.mean(movies_genre, axis=0) * 1, 2))[0])\n",
    "utl.barplot(attributes, np.array(scores) * 100, xlabel='Genre', ylabel='Percentage (%)', \n",
    "            title=\" \", rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Ratings: Importation et traitement des données\n",
    "\n",
    "La base de données comportant les évaluations des films effectuées par les usagers est constituée d'environ 100 mille lignes (une évaluation par ligne) où sont respectivement recensés le numéro d'identification de l'utilisateur, le numéro d'identification du film, l'évaluation associée et un marqueur de temps auquel le film a été visionné. Les ensembles d'entraînement et de test ont été fournis tels quels, c'est-à-dire que nous n'avons pas besoin de les construire nous-même, et comportent respectivement 80 et 20 mille évaluations.\n",
    "\n",
    "Pour des raisons pratiques, nous convertissons la base de données sous la forme d'une liste grâce à la fonction maison `convert`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(pd.read_csv('../data/ml-100k/u1.base', delimiter='\\t'), dtype='int')\n",
    "testing_set = np.array(pd.read_csv('../data/ml-100k/u1.test', delimiter='\\t'), dtype='int')\n",
    "\n",
    "train_set = utl.convert(training_set, nb_users, nb_movies)\n",
    "test_set = utl.convert(testing_set, nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons fait auparavant, nous pouvons obtenir quelques statistiques descriptives associées aux évaluations. Dans une premier temps, il pourrait être intéressant d'étudier les tendances moyennes des individus.\n",
    "\n",
    "##### Question 1\n",
    "\n",
    "1. Quelles autres types de statistiques pourraient être intéressantes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_set)\n",
    "shape = (len(train_set), len(train_set[0]))\n",
    "train_matrix.reshape(shape)\n",
    "train_matrix_bool = np.where(train_matrix > 0 , 1, 0)\n",
    "\n",
    "user_watch = np.sum(train_matrix_bool, axis=1)\n",
    "pd.DataFrame(user_watch).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec un petit histogramme..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.title('Empirical distribution of \\n the number of movies watched per user')\n",
    "plt.xlabel('Number of movies watched')\n",
    "plt.ylabel('Number of users')\n",
    "plt.hist(user_watch, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous présentons finalement quelques statistiques associées aux films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_frequency = np.mean(train_matrix_bool, axis=0)\n",
    "pd.DataFrame(movie_frequency).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2\n",
    "\n",
    "1. Quelles statistiques ou observations pourraient nous paraître pertinentes? Pourquoi? \n",
    "2. Quel type de statistique pourrait être plus approprié dans un tel contexte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Proportion of the population who watched the movie')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.hist(movie_frequency, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Étude des préférences individuelles en fonction du type de film\n",
    "\n",
    "Nous pourrions également nous intéresser au comportement d'un individu en particulier. Entre autres choses, nous pourrions étudier s'il y a un biais associé à son schème d'évaluation ou encore quelles sont ses préférences cinématographiques en fonction du score attribué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_user(data, movies_genre, user_id):\n",
    "    \n",
    "    ratings = data[user_id]\n",
    "    stats = np.zeros(6)\n",
    "    eva = np.zeros((6, movies_genre.shape[1]))\n",
    "\n",
    "    for k in np.arange(len(ratings)):\n",
    "        index = int(ratings[k])\n",
    "        stats[index] += 1\n",
    "        eva[index, :] = eva[index, :] + movies_genre[k]\n",
    "\n",
    "    return stats, eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "stats, eva = stats_user(train_set, movies_genre, user_id)\n",
    "utl.barplot(np.arange(5) + 1, stats[1:6] / sum(stats[1:6]), xlabel='Number of stars', ylabel='Percentage of movies (%)', \n",
    "            title=\" \", rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3\n",
    "\n",
    "1. Comment vérifier qu'il existe un biais associé au schème d'évaluation d'un individu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Création des sous-ensembles d'entraînement et de validation\n",
    "\n",
    "En apprentissage automatique, nous manipulons des <a href=\"https://blogs.nvidia.com/blog/2018/04/15/nvidia-research-image-translation/\">bases de données complexes</a> pour lesquelles nous tentons de définir des espaces de fonctions tout aussi complexes dans le but d'accomplir une tâche précise. Ceci étant, ces espaces de fonctions sont définis par un ensemble de paramètres dont le nombre tend à augmenter avec la complexité des données. Une fois l'espace défini par un ensemble de paramètres fixés, nous pouvons varier les différentes valeurs d'hyperparamètres afin d'explorer empiriquement les espaces de fonctions. Pour choisir l'ensemble des paramètres et d'hyperparamètres optimaux, nous définissons une métrique nous permettant d'évaluer le modèle; par exemple, à quel point l'image d'un chat nous paraît vraisemblable. \n",
    "\n",
    "Dans la mesure où nous voulons développer un modèle capable de généraliser, l'évaluation de ses performances, et donc la sélection des (hyperparamtètres) doit se faire sur un ensemble de données indépendant, mais issues de la même distribution, de l'ensemble sur lequel il a appris. Pareil ensemble porte le nom d'ensemble de validation.\n",
    "\n",
    "<b>! Remarque !</b> \n",
    "\n",
    "La notion d'ensemble d'entraînement et de test dans le cadre de système de recommandation est quelque peu différente de ce que l'on voit habituellement avec les problèmes dits supervisés. Si dans le cadre d'un problème supervisé, l'ensemble de test consiste essentiellement en de nouvelles observations (lire lignes d'un fichier) indépendantes des observations préalablement observées dans l'ensemble d'entraînement, le paradigme est sensiblement différent lorsque nous travaillons avec des systèmes de recommandation.\n",
    "\n",
    "Effectivement, et en raison du modèle mathématique sur lequel est basé les systèmes de recommandation, les données appartenant à l'ensemble de test ne sont pas liées à un nouvel individu, mais bien à de nouvelles évaluations, faites par le même ensemble d'individus, mais jusqu'alors inobservées. Dès lors, les données associées aux ensembles d'entraînement, de validation et de test ne sont plus indépendantes tel que supposé (la fameuse hypothèse <i> iid </i>); ce qui complique théoriquement les choses.\n",
    "\n",
    "Puisque le but de l'atelier n'est pas d'étudier la notion de biais associée au type de dépendance entre les différents évaluations dans les systèmes de recommendation, nous allons naïvement supposer que chacune des évaluations sont indépendantes les unes des autres. N'empêche, dans un cadre pratique, ignorer ce genre de considérations pourra éventuellement biaiser les algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data, ratio, tensor=False):\n",
    "    train = np.zeros((len(data), len(data[0]))).tolist()\n",
    "    valid = np.zeros((len(data), len(data[0]))).tolist()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if data[i][j] > 0:\n",
    "                if np.random.binomial(1, ratio, 1):\n",
    "                    train[i][j] = data[i][j]\n",
    "                else:\n",
    "                    valid[i][j] = data[i][j]\n",
    "\n",
    "    return [train, valid]\n",
    "\n",
    "train = split(train_set, 0.8)\n",
    "test = test_set\n",
    "\n",
    "pickle.dump(train, open('../data/saves/train', 'wb'))\n",
    "pickle.dump(test, open('../data/saves/test', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Système de recommandation: factorisation matricielle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Modèle\n",
    "\n",
    "La factorisation matricielle (FM) suppose que chaque évaluation observée $r_{ui}$ pour $1 \\leq u \\leq |U|$ et $1 \\leq i \\leq |I|$, où $|U|$ et $|I|$ dénotent respectivement le nombre total d'usagers et d'items, peut être estimée en considérant un modèle latent. Cette modélisation s'avère être une estimation $\\hat{r}_{ui}$ de l'observation associée $r_{ui}$ et est définie par:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_{u}, q_{i} \\rangle, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "où $\\langle \\cdot \\rangle$ symbolise le produit scalaire alors que $p_{u}$ et $q_{i}$ sont les représentations latentes sous forme vectorielle associées à l'usager $u$ et à l'item $i$. L'intuition derrière cette représentation suggère que chaque évaluation peut être estimée en considérant une caractérisation latente des usagers et des items. \n",
    "\n",
    "À titre d'exemple, fixons le nombre de variables latentes à trois et supposons encore que celles-ci sont associées à la popularité du film au box-office, sa durée et finalement son niveau de romantisme. Définissons l'usager $u$ comme étant un adolescent de 15 ans aimant les films populaires, relativement courts et très macabres. Nous pourrons modéliser le vecteur latent associé par:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{u} = [1, 0, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Supposons maintenant que le film $i$ s'avère être <i>Le Roi lion</i> auquel est associé la modélisation suivante:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_{i} = [1, 0.5, 0]^T.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "L'estimation de l'évaluation pour cet usager et item précis en fonction des représentations latentes sera donc:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{r}_{ui} =  \\langle p_u, q_i \\rangle = 1.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Le principal défi dans ce type de modélisation est de définir l'ensemble des vecteurs latents associés aux usagers, regroupés sous la forme matricielle par $\\mathbf{P}_{|U|\\times k} = [p_1, p_2, ..., p_k]$, et aux items, regroupés sous la forme matricielle par $\\mathbf{Q}_{|I|\\times k} = [q_1, q_2, ..., q_k]$. \n",
    " \n",
    "Puisque le problème initial consiste à présenter les estimations les plus justes, et donc à calculer $\\mathbf{P}$ et $\\mathbf{Q}$ de manière à minimiser la distance entre la totalité des évaluations observées $r_{ui}$ et leur estimation $\\hat{r}_{ui}$, nous pouvons définir la tâche à accomplir avec le problème d'optimisation suivant:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2 = \\underset{p, q}{\\operatorname{argmin}}  \\sum_{r_{ui} \\neq 0} (r_{ui} - \\langle p_u, q_i \\rangle)^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Au problème d'optimisation ci-dessus, nous ajouterons une contrainte sur les variables latentes, afin de forcer les vecteurs associés à avoir des composantes non nulles:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}, \\mathbf{Q} = \\underset{p, q}{\\operatorname{argmin}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "où $\\lambda$ est la régulasition, plus souvent nommé <i>weigth decay</i> en apprentissage profond ou encore multiplicateur de Lagrange en mathématiques. Bien que cette dernière remarque semble technique, notons simplement que des vecteurs latents ne présentant que très peu de valeurs nulles conduiront à leur tour à des évaluations prédites différentes de zéro. Dans la mesure où nous cherchons à proposer de nouvelles recommandations, ce type de contraintes sur les vecteurs latents nous paraît utile et nous évitera d'obtenir une matrice des estimations $\\hat{\\mathbf{R}}$ creuse.\n",
    "\n",
    "De façon générale, le problème d'optimisation ci-dessus, qui s'avère à factoriser une matrice creuse, ne peut se résoudre aussi facilement qu'à l'aide des moindres carrées comme c'est le cas dans le cadre de la régression linéaire par exemple. Deux méthodes seront présentées au cours de ce tutoriel pour estimer les matrices $\\mathbf{P}$ et $\\mathbf{Q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implémentation\n",
    "\n",
    "Afin de construire un système de recommandation basé sur la factorisation matricielle, nous devrons définir quelques fonctions nécessaires à ce type d'algorithme. Globalement, nous pouvons décliner l'algorithme de factorisation matricielle en trois étapes:\n",
    "<r>\n",
    "\n",
    "1. <b> Boucle d'apprentissage</b>: Passe en revue chacune des évaluations en boucle jusqu'à ce que le modèle estimé ne varie que légèrement d'une itération à l'autre en fonction d'un critère choisi.\n",
    "2. <b> Estimation</b>:  Estimation des matrices de facteurs $\\mathbf{P}$ et $\\mathbf{Q}$ respectivement associées aux users et aux items.\n",
    "3. <b> Évaluation</b>: Évaluation de la performance du modèle en fonction d'une métrique choisie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Boucle d'apprentissage\n",
    "\n",
    "La fonction consiste simplement à passer en boucle les différentes évaluations jusqu'à ce qu'un critère d'arrêt donné soit respecté.\n",
    "\n",
    "##### Question 4\n",
    "\n",
    "1. Dans le cas des estimateurs obtenus via la descente de gradient stochastique (SGD), quelle condition devrions-nous rajouter à la ligne 17? Et pourquoi?\n",
    "2. À la fin de chaque époque, quelle statistique serait-il préférable de calculer? Codez-la. Remarque : il est préférable d'initialiser des objets en début de fonction (voir ligne 6).\n",
    "3. Donnez deux raisons pour lesquelles ces statistiques sont utiles.\n",
    "4. Le critère d'arrêt pour éviter le surapprentissage à la ligne 30 est plutôt naïf. Pourquoi?\n",
    "5. Développez un nouveau critère d'arrêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_to_recommend(data, features=10, lr=0.0002, epochs=101, weigth_decay=0.02, stopping=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data: ensemble des évaluations\n",
    "      features: variables latentes\n",
    "      lr: learning rate pour la descente de gradient\n",
    "      epochs: nombre d'iterations ou boucles maximales à effectuer\n",
    "      weigth_decay: régularisation de type L2 afin de predire des valeurs differentes de 0\n",
    "      stopping: scalaire associé au critère d'arrête\n",
    "      \n",
    "    Returns:\n",
    "      P: matrice latente associée aux usagers\n",
    "      Q: matrice latente associée aux items\n",
    "      loss_train: un vecteur des différentes valeurs de la fonction perte après chaque itération sur train\n",
    "      loss_valid: un vecteur des différentes valeurs de la fonction perte après chaque itération pas sur valid\n",
    "      \"\"\"\n",
    "    train, valid = data[0], data[1]\n",
    "    nb_users, nb_items = len(train), len(train[0])\n",
    "\n",
    "    # Question 4.2: Listes à initialiser \n",
    "    # \n",
    "    \n",
    "    P = np.random.rand(nb_users, features) * 0.1\n",
    "    Q = np.random.rand(nb_items, features) * 0.1\n",
    "    \n",
    "    for k in range(epochs):        \n",
    "        for i in range(nb_users):\n",
    "            for j in range(nb_items):\n",
    "\n",
    "                # Question 4.1: Codez la condition sur la ligne ci-dessous.\n",
    "                # if ...\n",
    "                    eij = train[i][j] - prediction(P, Q, i, j)\n",
    "                    P, Q = sgd(eij, P, Q, i, j, features, lr)\n",
    "                               \n",
    "        # Question 4.2: Codez la statistique\n",
    "        #\n",
    "        #\n",
    "        \n",
    "        if k % 10 == 0:\n",
    "            print('Epoch : ', \"{:3.0f}\".format(k+1), ' | Train :', \"{:3.3f}\".format(loss_train[-1]), \n",
    "                  ' | Valid :', \"{:3.3f}\".format(loss_valid[-1]))\n",
    "\n",
    "        # Question 4.4: De la naïveté du critère d'arrêt\n",
    "        if abs(loss_train[-1]) < stopping:\n",
    "            break\n",
    "            \n",
    "        # Question 4.5 : Nouveau critère d'arrêt\n",
    "        # if ... :\n",
    "            break\n",
    "        \n",
    "    return P, Q, loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Fonction de coût\n",
    "\n",
    "La fonction de coût joue un rôle déterminant dans la construction d'un modèle prédictif. En effect, c'est cette même fonction de coût que nous essaierons de minimiser (ou maximiser c'est selon) en ajustant de façon itérative les valeurs des matrices latentes.\n",
    "\n",
    "Dans la mesure où l'on considère que les évaluations considérées varient entre 1 et 5, l'erreur quadratique moyenne (EQM) semble une première option intéressante. Formellement, dans le cadre d'un système de recommandation, nous définierons l'EQM ainsi : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "EQM (\\mathbf{R}, \\hat{\\mathbf{R}}) = \\frac{1}{n} \\sum_{r_{ui} \\neq 0} (r_{ui} - \\hat{r}_{ui})^2, \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "où $\\mathbf{R}$ et $\\hat{\\mathbf{R}}$ sont respectivement les matrices des évaluations observées et prédites, <i>n</i> est le nombre total d'évaluations estimées. De la même façon, $r_{ui}$ et $\\hat{r}_{ui}$ sont des scalaires associés respectivement à l'évaluation observée et l'évaluation estimée de l'usager $u$ pour l'item $i$.\n",
    "\n",
    "##### Question 5\n",
    "\n",
    "1. Supposons que nous voulons prédire l'évaluation de l'individu <i>i</i> pour le film <i>j</i>, comment devrions-nous nous y prendre? Implémentez la fonction prédiction.\n",
    "2. Un détail important est manquant dans la fonction `loss` suivante. En quoi cette erreur est fondamentale? Corrigez-la. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5.1: Implémentez la fonction prédiction\n",
    "def prediction(P, Q, i, j):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       P: matrice des usagers\n",
    "       Q: matrice des items\n",
    "       i: indice associé à l'usager i\n",
    "       j: indice associé à l'item j\n",
    "    \n",
    "    Returns:\n",
    "       pred: l'évaluation prédite de l'usager i pour l'item j\n",
    "       \"\"\"\n",
    "    \n",
    "    # return ...\n",
    "\n",
    "def loss(data, P, Q):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       data: données\n",
    "       P: matrice des usagers\n",
    "       Q: matrice des items\n",
    "       \n",
    "    Returns:\n",
    "        EQM: la moyenne observée des erreurs au carré\n",
    "        \"\"\"\n",
    "    \n",
    "    errors_sum, nb_evaluations = 0., 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            \n",
    "            # Question 5.2: Un détail important\n",
    "            #\n",
    "                errors_sum += pow(data[i][j] - prediction(P, Q, i, j), 2)\n",
    "                nb_evaluations += 1\n",
    "    return errors_sum / nb_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Estimation\n",
    "\n",
    "La méthode d'estimation des paramètres du modèle est directement associée à la fonction de coût que nous essayons de minimiser. Avec la factorisation matricielle, deux techniques d'estimation sont disponibles afin de calculer les matrices latentes $\\mathbf{P}$ et $\\mathbf{Q}$ respectivement associées aux usagers et aux items. Dans tous les cas, ces techniques font appel à la linéarité du modèle de factorisation matriciel.\n",
    "\n",
    "#### Descente du gradient\n",
    "\n",
    "Dans un premier temps, nous implémentons la descente stochastique du gradient (SGD): une méthode itérative passant en revue l'ensemble des évaluations non nulles pour chacun des usagers.\n",
    "\n",
    "Formellement, et en se rappelant que la fonction que nous essayons de minimiser est:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{p, q}{\\operatorname{min}} L(\\mathbf{R}, \\lambda) = \\underset{p, q}{\\operatorname{min}} \\sum_{r_{ui} \\neq 0} \\{(r_{ui} - \\langle p_u, q_i \\rangle)^2 + \\lambda \\cdot (||p_u||^2 + ||q_i||^2)\\},\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "nous calculons que les gradients de la précédente équation en fonction de $p_u$ et $q_i$ sont:\n",
    "\n",
    "$$\n",
    "\\nabla_{p_{u}} L(\\mathbf{R}, \\lambda) =  \\epsilon_{ui} \\cdot q_{i} - \\lambda \\cdot p_{u} \\quad \\text{et} \\quad \n",
    "\\nabla_{q_{i}} L(\\mathbf{R}, \\lambda) =  \\epsilon_{ui} \\cdot p_{u} - \\lambda \\cdot q_{i},\n",
    "$$\n",
    "\n",
    "où\n",
    "\n",
    "$$\n",
    "\\epsilon_{ui} = r_{ui} - \\hat{r}_{ui}. \n",
    "$$\n",
    "\n",
    "Enfin, pour chaque itération, et dans la mesure où l'évaluation observée est non-nulle, chacune des mises à jours des vecteurs latentes pourra se faire ainsi:\n",
    "\n",
    "$$\n",
    "p_{u}^{(t+1)} \\leftarrow p_{u}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot q_{i}^{(t)} - \\lambda \\cdot p_{u}^{(t)}) \\\\\n",
    "q_{i}^{(t+1)} \\leftarrow q_{i}^{(t)} + \\gamma \\cdot (\\epsilon_{ui} \\cdot p_{u}^{(t)} - \\lambda \\cdot q_{i}^{(t)}),\n",
    "$$\n",
    "\n",
    "où $p_{u}^{(t+1)}$ dénote la valeur de $p_{u}$ après la $t + 1$ ième itération et où $\\gamma$ est le pas d'apprentissage (<i>learning rate</i>) de la descente. \n",
    "\n",
    "#### Remarques sur les moindres carrées alternés\n",
    "\n",
    "La deuxième technique est basée sur les moindres carrés alternés (ALS). Cette méthode a ceci d'élégant qu'elle permet une forme analytique. Nous ne l'implémenterons pas dans le cadre de cet atelier.\n",
    "\n",
    "##### Question 6\n",
    "\n",
    "1. En considérant les équations associées à la descente du gradient, complétez la fonction `sgd` afin d'implémentez la descente stochastique du gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(error, P, Q, id_user, id_item, features, lr):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       error: différence entre l'évaluation observée et celle prédite (dans cet ordre)\n",
    "       P: matrix of users\n",
    "       Q: matrix of items\n",
    "       id_user: id_user\n",
    "       id_item: id_item\n",
    "       features: nombre de variables latente\n",
    "       lr: pas d'apprentissage pour la descente du gradient\n",
    "       \n",
    "    Returns:\n",
    "        P: la nouvelle estimation de P\n",
    "        Q: la nouvelle estimation de Q\n",
    "        \"\"\"\n",
    "    # for :\n",
    "        #\n",
    "        #\n",
    "        \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Entraînement du modèle\n",
    "\n",
    "La factorisation matricielle maintenant implémentée, nous pouvons commencer à entraîner le modèle avec différents paramètres et hyperparamètres. L'idée ici n'est pas d'ajuster les paramètres de façon telle à obtenir le meilleur modèle possible, mais simplement de comprendre le rôle que ceux-ci peuvent jouer, tant du point de vue du surappentissage que du temps de calcul. En fait, ici, il n'y a que très peu de mauvaises réponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 5\n",
    "lr = 0.02\n",
    "epochs = 101\n",
    "weigth_decay = 0.02\n",
    "stopping = 0.01\n",
    "\n",
    "P, Q, loss_train, loss_valid = learn_to_recommend(train, features, lr, epochs, weigth_decay, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois, le modèle entraîné, nous pouvons visualiser les différentes courbes d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(len(loss_train)))\n",
    "k=0\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.plot(x[-k:], loss_train[-k:], 'r', label=\"Train\")\n",
    "plt.plot(x[-k:], loss_valid[-k:], 'g', label=\"Validation\")\n",
    "plt.title('Learning curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "leg = plt.legend(loc='best', shadow=True, fancybox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7\n",
    "\n",
    "1. Était-ce vraiment nécessaire de calculer autant d'époques?\n",
    "2. En nous inspirant de ces courbes, quel critère d'arrêt pourrions-nous développer?\n",
    "3. En quoi est-il plus pertinent que celui défini dans la boucle d'apprentissage?\n",
    "4. Implémentez-le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous pouvons évaluer les performances de notre modèle sur l'ensemble test.\n",
    "\n",
    "##### Question 8\n",
    "\n",
    "1. Implémentez la procédure.\n",
    "2. En quoi est-ce pertinent d'évaluer les performances sur un tel ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Analyse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Exploration des couches latentes\n",
    "\n",
    "Grâce à la factorisation matricielle, il est possible d'explorer les différentes variables latentes associées aux usagers et aux items. De par la nature des matrices $\\mathbf{P}$ et $\\mathbf{Q}$, l'exploration des <i>k</i> variables latentes associées aux colonnes de $\\mathbf{P}$ et $\\mathbf{Q}$ pourraient s'avérer intéressante.\n",
    "\n",
    "Nous proposons donc une fonction facilitant l'exploration des variables latentes. Considérant le commentaire en lien avec la fréquence de visualisation des films à la Section 1.4.3, nous allons seulement nous intéresser aux films présentant une fréquence de visualisation supérieure à un seuil choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration(object_name, matrix, freq, factor, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       object_name: vecteur des noms des films\n",
    "       matrix: matrice latente associée aux films\n",
    "       freq: fréquence de visionnement minimale au-delà de laquelle nous pouvons considerer le film dans l'analyse\n",
    "       factor: le numéro de la variable latente que l'on veut étudier\n",
    "       k: nombre de films en sortie = 3*k - 1\n",
    "       \n",
    "    Returns:\n",
    "        names: le titre des films\n",
    "        scores: l'évaluation prédite associée\n",
    "        \"\"\"\n",
    "    \n",
    "    values = matrix[:, factor] * freq\n",
    "    names, scores = utl.rearrange(object_name, values)\n",
    "    nonzero = np.nonzero(scores)\n",
    "    \n",
    "    start = nonzero[0][0]    \n",
    "    center = int((len(object_name) - start) / 2)\n",
    "    \n",
    "    names = names[start: (start+k) ] + names[(start+center - 1):(start+ center + 2)] + names[-k:]\n",
    "    scores = scores[start : (start+k) ] + scores[(start+center - 1):(start+ center + 2)] + scores[-k:]\n",
    "    \n",
    "    return names, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appellons la fonction et visualisons les résultats.\n",
    "\n",
    "##### Question 9\n",
    "\n",
    "1. Est-ce que certaines variables latentes peuvent être interprétables?\n",
    "2. Qu'arrivera-t-il si nous augmentons le nombre de variables latentes? Si nous le diminuons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "factor = 0\n",
    "threshold = 0.1\n",
    "names, scores = exploration(movies['Title'], Q, np.where(movie_frequency > threshold, 1, 0), factor, k)\n",
    "\n",
    "df = pd.DataFrame(np.matrix((names, scores)).T, (np.arange(len(scores)) + 1).tolist())\n",
    "df.columns = ['Title', 'Latent factor']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Applications \n",
    "\n",
    "L'un des objectifs premier des systèmes de recommandation est d'effectuer de recommandations (!) personnalisées pour chacun des utilisateurs. Dès lors, il pourrait être intéressant d'étudier les recommandations effectuées par notre modèle pour un individu spécifique. Naturellement, les recommendations faites ne suggèrent que des films non visionnés par l'usager.\n",
    "\n",
    "##### Question 10\n",
    "\n",
    "1. Nous allons maintenant, pour un usager choisi, effectuer les dix meilleures recommandations associées. Pour ce faire, nous allons procéder par étapes simples tel que présentées dans le code. Notez que la fonction maison `rearrange` présentée ci-dessous pourrait vous être utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:  \n",
    "Pour un ensemble de valeurs numériques (le paramètre ratings) auquel est associé un autre ensemble de même \n",
    "longueur (le paramètre names) numérique ou non, la fonction trie en ordre croissant ratings tout en conservant\n",
    "la correspondance avec l'objet names\n",
    "\n",
    "ex:\n",
    "a, b = ['a', 'b', 'c'], [6,1,3]\n",
    "a, b = rearrange(a, b)\n",
    "print(\"a:\", a, \"\\nb:\", b)\n",
    "a: ['b', 'c', 'a'] \n",
    "b: [1, 3, 6]\n",
    "\"\"\"\n",
    "\n",
    "def rearrange(names, ratings):\n",
    "    attribute, scores = [], []\n",
    "    ranking = np.argsort(ratings)\n",
    "\n",
    "    for k in np.arange(len(ranking)):\n",
    "        attribute.append(names[ranking[k]])\n",
    "        scores.append(ratings[ranking[k]])\n",
    "\n",
    "    return attribute, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "top_what = 10\n",
    "\n",
    "# Étape 1: Définir quels films ont déjà été visionnés dans chacun des sous ensembles\n",
    "\n",
    "# Étape 2: Calculez l'ensemble de évaluations prédites pour l'individu choisi\n",
    "\n",
    "# Étape 3: Considérez seulement les évaluations associées aux ensembles d'entraînement et de validation\n",
    "\n",
    "# Étape 4: Réorganisez les estimations sur les différents ensembles de manière à proposer les 10 meilleures\n",
    "#          recommandations\n",
    "\n",
    "# Présentations des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En manipulant rapidement le code ci-dessus, on s'apperçoit que les évaluations sur les prédictions sur les ensembles d'entraînement, de validation et même de test sont vraisemblables. Effectivement, ces dernières se situent entre 0 et 5, tout semble alors en ordre, ce qui nous paraît \"normal\" puisque les EQM sur les ensembles d'entraînement et de validation n'étaient pas particulièrement élevées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait, ça pourrait être intéressant de proposer à l'usager des films en fonction de ces préférences du moment en fonction du genre.\n",
    "\n",
    "##### Question 11\n",
    "\n",
    "1. Écrivez une fonction permettant d'effectuer pareille tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(user_id, data, P, Q, movies_genre, genre, new):\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    return np.array(predictions) * np.array(genre.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = \"Animation\"\n",
    "top_what = 5\n",
    "\n",
    "# Calcul et réorganisation des recommandations\n",
    "estimate = recommendations(user_id, train, P, Q, movies_genre, genre, False)\n",
    "suggestions, scores = rearrange(np.array(movies['Title']), estimate)\n",
    "\n",
    "# Présentations des recommandations\n",
    "df = pd.DataFrame(np.matrix((suggestions[-top_what:], scores[-top_what:])).T, (np.arange(top_what) + 1).tolist())\n",
    "df.columns = ['Title', 'Predicted rating']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
